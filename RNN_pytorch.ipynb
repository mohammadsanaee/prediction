{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21ad201b",
      "metadata": {
        "id": "21ad201b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c1f58b5c",
      "metadata": {
        "id": "c1f58b5c"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/mohammadsanaee/prediction/main/df_daily.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "59743b61",
      "metadata": {
        "id": "59743b61",
        "outputId": "fc6cb42d-ec15-4da9-df9c-2c49eac7b23c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0        date  price_pesos  exchange  price_usd  gas_price  \\\n",
              "0           0  2006-06-26  3696.799565   53.4002  69.228197        7.1   \n",
              "1           1  2006-06-27  4494.720000   53.4750  84.052735        7.1   \n",
              "2           2  2006-06-28  3419.524583   53.5800  63.820914        7.1   \n",
              "3           3  2006-06-29  2904.085417   53.5502  54.231084        7.1   \n",
              "4           4  2006-06-30  4178.192500   53.1102  78.670246        7.1   \n",
              "\n",
              "   coal_price   gas_rate  coal_rate  log_price  \n",
              "0    2.100882   9.750450  32.951961   3.426412  \n",
              "1    2.100882  11.838413  40.008299        NaN  \n",
              "2    2.100882   8.988861  30.378146   3.471716  \n",
              "3    2.100882   7.638181  25.813478   3.346610  \n",
              "4    2.100882  11.080316  37.446286   3.480780  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5f910fdf-aa2f-496c-9fe0-14735d16beff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>date</th>\n",
              "      <th>price_pesos</th>\n",
              "      <th>exchange</th>\n",
              "      <th>price_usd</th>\n",
              "      <th>gas_price</th>\n",
              "      <th>coal_price</th>\n",
              "      <th>gas_rate</th>\n",
              "      <th>coal_rate</th>\n",
              "      <th>log_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2006-06-26</td>\n",
              "      <td>3696.799565</td>\n",
              "      <td>53.4002</td>\n",
              "      <td>69.228197</td>\n",
              "      <td>7.1</td>\n",
              "      <td>2.100882</td>\n",
              "      <td>9.750450</td>\n",
              "      <td>32.951961</td>\n",
              "      <td>3.426412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2006-06-27</td>\n",
              "      <td>4494.720000</td>\n",
              "      <td>53.4750</td>\n",
              "      <td>84.052735</td>\n",
              "      <td>7.1</td>\n",
              "      <td>2.100882</td>\n",
              "      <td>11.838413</td>\n",
              "      <td>40.008299</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2006-06-28</td>\n",
              "      <td>3419.524583</td>\n",
              "      <td>53.5800</td>\n",
              "      <td>63.820914</td>\n",
              "      <td>7.1</td>\n",
              "      <td>2.100882</td>\n",
              "      <td>8.988861</td>\n",
              "      <td>30.378146</td>\n",
              "      <td>3.471716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2006-06-29</td>\n",
              "      <td>2904.085417</td>\n",
              "      <td>53.5502</td>\n",
              "      <td>54.231084</td>\n",
              "      <td>7.1</td>\n",
              "      <td>2.100882</td>\n",
              "      <td>7.638181</td>\n",
              "      <td>25.813478</td>\n",
              "      <td>3.346610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2006-06-30</td>\n",
              "      <td>4178.192500</td>\n",
              "      <td>53.1102</td>\n",
              "      <td>78.670246</td>\n",
              "      <td>7.1</td>\n",
              "      <td>2.100882</td>\n",
              "      <td>11.080316</td>\n",
              "      <td>37.446286</td>\n",
              "      <td>3.480780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f910fdf-aa2f-496c-9fe0-14735d16beff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5f910fdf-aa2f-496c-9fe0-14735d16beff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5f910fdf-aa2f-496c-9fe0-14735d16beff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "551bb891",
      "metadata": {
        "id": "551bb891"
      },
      "outputs": [],
      "source": [
        "df1 = df.reset_index()['price_pesos']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8ac11941",
      "metadata": {
        "id": "8ac11941",
        "outputId": "8dc14ffd-f885-4a98-a7d3-c10f8662f03f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f83cef84fd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9fb94ac1",
      "metadata": {
        "id": "9fb94ac1"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scalar = MinMaxScaler(feature_range=(0,1))\n",
        "df1 = scalar.fit_transform(np.array(df1).reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2ac7fb6b",
      "metadata": {
        "id": "2ac7fb6b",
        "outputId": "d4b31d20-05fc-49aa-c99c-f070e483b583",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.09346681],\n",
              "       [0.11364076],\n",
              "       [0.08645642],\n",
              "       ...,\n",
              "       [0.05852367],\n",
              "       [0.0581322 ],\n",
              "       [0.04859631]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "34a2ab11",
      "metadata": {
        "id": "34a2ab11"
      },
      "outputs": [],
      "source": [
        "training_size=int(len(df1)*0.80)\n",
        "test_size=len(df1)-training_size\n",
        "train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "39a57103",
      "metadata": {
        "id": "39a57103"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset , DataLoader\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self,data,seq_len = 100):\n",
        "        self.data = data\n",
        "        self.data = torch.from_numpy(data).float().view(-1)\n",
        "        self.seq_len = seq_len\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)-self.seq_len-1\n",
        "\n",
        "    def __getitem__(self, index) :\n",
        "        return  self.data[index : index+self.seq_len] , self.data[index+self.seq_len]\n",
        "\n",
        "\n",
        "train_dataset = StockDataset(train_data) \n",
        "test_dataset = StockDataset(test_data) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1ded5593",
      "metadata": {
        "id": "1ded5593"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_dataset,batch_size,drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset,batch_size , drop_last=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "76ae64ea",
      "metadata": {
        "id": "76ae64ea"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class Lstm_model(nn.Module):\n",
        "    def __init__(self, input_dim , hidden_size , num_layers):\n",
        "        super(Lstm_model, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size=input_dim , hidden_size = hidden_size , num_layers= num_layers )\n",
        "        self.fc = nn.Linear(hidden_size,1)\n",
        "\n",
        "    def forward(self,x,hn,cn):\n",
        "        out , (hn,cn) = self.lstm(x , (hn,cn))\n",
        "        final_out = self.fc(out[-1])\n",
        "        return final_out,hn,cn\n",
        "\n",
        "    def predict(self,x):\n",
        "        hn,cn  = self.init()\n",
        "        final_out = self.fc(out[-1])\n",
        "        return final_out\n",
        "\n",
        "    def init(self):\n",
        "        h0 =  torch.zeros(self.num_layers , batch_size , self.hidden_size).to(device)\n",
        "        c0 =  torch.zeros(self.num_layers , batch_size , self.hidden_size).to(device)\n",
        "        return h0 , c0\n",
        "\n",
        "\n",
        "input_dim = 1 \n",
        "hidden_size = 50\n",
        "num_layers = 3\n",
        "\n",
        "model = Lstm_model(input_dim , hidden_size , num_layers).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "# Initialize lists to store training and test losses\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "def train(dataloader):\n",
        "    hn , cn = model.init()\n",
        "    model.train()\n",
        "    for batch , item in enumerate(dataloader):\n",
        "        x , y = item\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        out , hn , cn = model(x.reshape(100,batch_size,1),hn,cn)\n",
        "        loss = loss_fn(out.reshape(batch_size) , y)\n",
        "        hn = hn.detach()\n",
        "        cn = cn.detach()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch == len(dataloader)-1:\n",
        "            loss = loss.item()\n",
        "            train_losses.append(loss)\n",
        "            print(f\"train loss: {loss:>7f} \")\n",
        "            \n",
        "def test(dataloader):\n",
        "    hn , cn = model.init()\n",
        "    model.eval()\n",
        "    for batch , item in enumerate(dataloader):\n",
        "        x , y = item\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        out , hn , cn = model(x.reshape(100,batch_size,1),hn,cn)\n",
        "        loss = loss_fn(out.reshape(batch_size) , y)\n",
        "       \n",
        "        if batch == len(dataloader)-1:\n",
        "            loss = loss.item()\n",
        "            test_losses.append(loss)\n",
        "            print(f\"test loss: {loss:>7f} \")\n",
        "            \n",
        "# Training loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    train(train_dataloader)\n",
        "    test(test_dataloader)\n",
        "  \n",
        "  # if epoch %50 == 0:\n",
        "  #     print(f\"epoch {epoch} \")\n",
        "  #     train(train_dataloader)\n",
        "  #     test(test_dataloader)\n",
        "  # else:\n",
        "  #   train(train_dataloader)\n",
        "  #   test(test_dataloader)\n",
        "\n",
        "# Plotting the learning curve\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Rv4rF_lXv3M6",
        "outputId": "2b2a2d5c-fb25-4c97-a1e8-27633578cef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "Rv4rF_lXv3M6",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 0.000075 \n",
            "test loss: 0.000781 \n",
            "train loss: 0.000053 \n",
            "test loss: 0.000776 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.000805 \n",
            "train loss: 0.000058 \n",
            "test loss: 0.000815 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000722 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000817 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.000773 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000825 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000753 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.000718 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000764 \n",
            "train loss: 0.000051 \n",
            "test loss: 0.000720 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.000735 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.000767 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000811 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000749 \n",
            "train loss: 0.000072 \n",
            "test loss: 0.000837 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000762 \n",
            "train loss: 0.000080 \n",
            "test loss: 0.000877 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000743 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.000845 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000783 \n",
            "train loss: 0.000070 \n",
            "test loss: 0.000862 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000808 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.000837 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000824 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000821 \n",
            "train loss: 0.000058 \n",
            "test loss: 0.000820 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000844 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.000795 \n",
            "train loss: 0.000058 \n",
            "test loss: 0.000852 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000774 \n",
            "train loss: 0.000054 \n",
            "test loss: 0.000834 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000725 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000804 \n",
            "train loss: 0.000053 \n",
            "test loss: 0.000738 \n",
            "train loss: 0.000051 \n",
            "test loss: 0.000815 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000786 \n",
            "train loss: 0.000054 \n",
            "test loss: 0.000756 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.000844 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000779 \n",
            "train loss: 0.000053 \n",
            "test loss: 0.000849 \n",
            "train loss: 0.000054 \n",
            "test loss: 0.000782 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.000845 \n",
            "train loss: 0.000052 \n",
            "test loss: 0.000760 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.000812 \n",
            "train loss: 0.000051 \n",
            "test loss: 0.000768 \n",
            "train loss: 0.000053 \n",
            "test loss: 0.000857 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000795 \n",
            "train loss: 0.000049 \n",
            "test loss: 0.000884 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.000814 \n",
            "train loss: 0.000063 \n",
            "test loss: 0.000890 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000751 \n",
            "train loss: 0.000081 \n",
            "test loss: 0.000921 \n",
            "train loss: 0.000074 \n",
            "test loss: 0.000819 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.000883 \n",
            "train loss: 0.000078 \n",
            "test loss: 0.000778 \n",
            "train loss: 0.000085 \n",
            "test loss: 0.000870 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000797 \n",
            "train loss: 0.000111 \n",
            "test loss: 0.000936 \n",
            "train loss: 0.000042 \n",
            "test loss: 0.000801 \n",
            "train loss: 0.000085 \n",
            "test loss: 0.000827 \n",
            "train loss: 0.000053 \n",
            "test loss: 0.000826 \n",
            "train loss: 0.000111 \n",
            "test loss: 0.000898 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.000838 \n",
            "train loss: 0.000166 \n",
            "test loss: 0.000930 \n",
            "train loss: 0.000091 \n",
            "test loss: 0.000890 \n",
            "train loss: 0.000205 \n",
            "test loss: 0.000952 \n",
            "train loss: 0.000190 \n",
            "test loss: 0.000936 \n",
            "train loss: 0.000211 \n",
            "test loss: 0.000925 \n",
            "train loss: 0.000290 \n",
            "test loss: 0.000994 \n",
            "train loss: 0.000423 \n",
            "test loss: 0.000914 \n",
            "train loss: 0.000187 \n",
            "test loss: 0.000922 \n",
            "train loss: 0.000598 \n",
            "test loss: 0.000832 \n",
            "train loss: 0.000231 \n",
            "test loss: 0.000898 \n",
            "train loss: 0.001045 \n",
            "test loss: 0.000812 \n",
            "train loss: 0.000586 \n",
            "test loss: 0.000865 \n",
            "train loss: 0.000863 \n",
            "test loss: 0.000933 \n",
            "train loss: 0.000196 \n",
            "test loss: 0.000870 \n",
            "train loss: 0.000770 \n",
            "test loss: 0.000889 \n",
            "train loss: 0.000269 \n",
            "test loss: 0.000872 \n",
            "train loss: 0.000223 \n",
            "test loss: 0.000893 \n",
            "train loss: 0.000143 \n",
            "test loss: 0.000911 \n",
            "train loss: 0.000146 \n",
            "test loss: 0.000909 \n",
            "train loss: 0.000114 \n",
            "test loss: 0.000934 \n",
            "train loss: 0.000138 \n",
            "test loss: 0.000900 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000939 \n",
            "train loss: 0.000110 \n",
            "test loss: 0.000917 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000927 \n",
            "train loss: 0.000086 \n",
            "test loss: 0.000908 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.000930 \n",
            "train loss: 0.000080 \n",
            "test loss: 0.000909 \n",
            "train loss: 0.000069 \n",
            "test loss: 0.000954 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000928 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.000948 \n",
            "train loss: 0.000063 \n",
            "test loss: 0.000893 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000901 \n",
            "train loss: 0.000087 \n",
            "test loss: 0.000913 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000881 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.000923 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.000924 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000865 \n",
            "train loss: 0.000083 \n",
            "test loss: 0.000828 \n",
            "train loss: 0.000115 \n",
            "test loss: 0.000808 \n",
            "train loss: 0.000106 \n",
            "test loss: 0.000830 \n",
            "train loss: 0.000104 \n",
            "test loss: 0.000769 \n",
            "train loss: 0.000128 \n",
            "test loss: 0.000782 \n",
            "train loss: 0.000090 \n",
            "test loss: 0.000783 \n",
            "train loss: 0.000082 \n",
            "test loss: 0.000790 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.000790 \n",
            "train loss: 0.000063 \n",
            "test loss: 0.000850 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000843 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000863 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000873 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.000886 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.000897 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.000904 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000911 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000917 \n",
            "train loss: 0.000058 \n",
            "test loss: 0.000923 \n",
            "train loss: 0.000058 \n",
            "test loss: 0.000927 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.000932 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.000935 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000937 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000938 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.000937 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000936 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000933 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.000926 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000923 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.000904 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000919 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.000907 \n",
            "train loss: 0.000069 \n",
            "test loss: 0.000897 \n",
            "train loss: 0.000058 \n",
            "test loss: 0.000904 \n",
            "train loss: 0.000070 \n",
            "test loss: 0.000874 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.000884 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000856 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.000857 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.000854 \n",
            "train loss: 0.000080 \n",
            "test loss: 0.000871 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.000835 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.000867 \n",
            "train loss: 0.000090 \n",
            "test loss: 0.000825 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000861 \n",
            "train loss: 0.000082 \n",
            "test loss: 0.000806 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000863 \n",
            "train loss: 0.000082 \n",
            "test loss: 0.000841 \n",
            "train loss: 0.000078 \n",
            "test loss: 0.000872 \n",
            "train loss: 0.000081 \n",
            "test loss: 0.000834 \n",
            "train loss: 0.000086 \n",
            "test loss: 0.000838 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.000793 \n",
            "train loss: 0.000086 \n",
            "test loss: 0.000796 \n",
            "train loss: 0.000081 \n",
            "test loss: 0.000809 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.000854 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.000824 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.000911 \n",
            "train loss: 0.000109 \n",
            "test loss: 0.000827 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000855 \n",
            "train loss: 0.000111 \n",
            "test loss: 0.000796 \n",
            "train loss: 0.000085 \n",
            "test loss: 0.000797 \n",
            "train loss: 0.000112 \n",
            "test loss: 0.000819 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.000820 \n",
            "train loss: 0.000101 \n",
            "test loss: 0.000879 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.000836 \n",
            "train loss: 0.000093 \n",
            "test loss: 0.000897 \n",
            "train loss: 0.000085 \n",
            "test loss: 0.000853 \n",
            "train loss: 0.000099 \n",
            "test loss: 0.000904 \n",
            "train loss: 0.000096 \n",
            "test loss: 0.000863 \n",
            "train loss: 0.000086 \n",
            "test loss: 0.000964 \n",
            "train loss: 0.000083 \n",
            "test loss: 0.000806 \n",
            "train loss: 0.000125 \n",
            "test loss: 0.000841 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000724 \n",
            "train loss: 0.000097 \n",
            "test loss: 0.000788 \n",
            "train loss: 0.000094 \n",
            "test loss: 0.000801 \n",
            "train loss: 0.000110 \n",
            "test loss: 0.000887 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.000801 \n",
            "train loss: 0.000104 \n",
            "test loss: 0.000874 \n",
            "train loss: 0.000090 \n",
            "test loss: 0.000809 \n",
            "train loss: 0.000074 \n",
            "test loss: 0.000875 \n",
            "train loss: 0.000089 \n",
            "test loss: 0.000853 \n",
            "train loss: 0.000053 \n",
            "test loss: 0.000844 \n",
            "train loss: 0.000103 \n",
            "test loss: 0.000935 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000750 \n",
            "train loss: 0.000112 \n",
            "test loss: 0.000789 \n",
            "train loss: 0.000083 \n",
            "test loss: 0.000742 \n",
            "train loss: 0.000080 \n",
            "test loss: 0.000921 \n",
            "train loss: 0.000072 \n",
            "test loss: 0.000790 \n",
            "train loss: 0.000101 \n",
            "test loss: 0.001019 \n",
            "train loss: 0.000084 \n",
            "test loss: 0.000936 \n",
            "train loss: 0.000102 \n",
            "test loss: 0.000878 \n",
            "train loss: 0.000117 \n",
            "test loss: 0.000909 \n",
            "train loss: 0.000087 \n",
            "test loss: 0.001069 \n",
            "train loss: 0.000134 \n",
            "test loss: 0.000990 \n",
            "train loss: 0.000083 \n",
            "test loss: 0.001037 \n",
            "train loss: 0.000093 \n",
            "test loss: 0.000906 \n",
            "train loss: 0.000079 \n",
            "test loss: 0.000968 \n",
            "train loss: 0.000108 \n",
            "test loss: 0.000823 \n",
            "train loss: 0.000119 \n",
            "test loss: 0.000860 \n",
            "train loss: 0.000095 \n",
            "test loss: 0.000795 \n",
            "train loss: 0.000074 \n",
            "test loss: 0.000925 \n",
            "train loss: 0.000104 \n",
            "test loss: 0.000931 \n",
            "train loss: 0.000087 \n",
            "test loss: 0.000948 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.000959 \n",
            "train loss: 0.000078 \n",
            "test loss: 0.001011 \n",
            "train loss: 0.000105 \n",
            "test loss: 0.000918 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000971 \n",
            "train loss: 0.000091 \n",
            "test loss: 0.000982 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.000964 \n",
            "train loss: 0.000127 \n",
            "test loss: 0.000933 \n",
            "train loss: 0.000119 \n",
            "test loss: 0.001044 \n",
            "train loss: 0.000121 \n",
            "test loss: 0.000958 \n",
            "train loss: 0.000096 \n",
            "test loss: 0.000877 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.000796 \n",
            "train loss: 0.000096 \n",
            "test loss: 0.000919 \n",
            "train loss: 0.000099 \n",
            "test loss: 0.000923 \n",
            "train loss: 0.000113 \n",
            "test loss: 0.000913 \n",
            "train loss: 0.000103 \n",
            "test loss: 0.001016 \n",
            "train loss: 0.000108 \n",
            "test loss: 0.000956 \n",
            "train loss: 0.000098 \n",
            "test loss: 0.000876 \n",
            "train loss: 0.000087 \n",
            "test loss: 0.000897 \n",
            "train loss: 0.000136 \n",
            "test loss: 0.000909 \n",
            "train loss: 0.000109 \n",
            "test loss: 0.001057 \n",
            "train loss: 0.000124 \n",
            "test loss: 0.000978 \n",
            "train loss: 0.000087 \n",
            "test loss: 0.000971 \n",
            "train loss: 0.000109 \n",
            "test loss: 0.000915 \n",
            "train loss: 0.000120 \n",
            "test loss: 0.000966 \n",
            "train loss: 0.000096 \n",
            "test loss: 0.000867 \n",
            "train loss: 0.000126 \n",
            "test loss: 0.000918 \n",
            "train loss: 0.000116 \n",
            "test loss: 0.000892 \n",
            "train loss: 0.000090 \n",
            "test loss: 0.000987 \n",
            "train loss: 0.000100 \n",
            "test loss: 0.000874 \n",
            "train loss: 0.000108 \n",
            "test loss: 0.000929 \n",
            "train loss: 0.000103 \n",
            "test loss: 0.000972 \n",
            "train loss: 0.000138 \n",
            "test loss: 0.000932 \n",
            "train loss: 0.000116 \n",
            "test loss: 0.000846 \n",
            "train loss: 0.000105 \n",
            "test loss: 0.000984 \n",
            "train loss: 0.000103 \n",
            "test loss: 0.000858 \n",
            "train loss: 0.000102 \n",
            "test loss: 0.000869 \n",
            "train loss: 0.000096 \n",
            "test loss: 0.000889 \n",
            "train loss: 0.000116 \n",
            "test loss: 0.000953 \n",
            "train loss: 0.000115 \n",
            "test loss: 0.000872 \n",
            "train loss: 0.000086 \n",
            "test loss: 0.000964 \n",
            "train loss: 0.000114 \n",
            "test loss: 0.000841 \n",
            "train loss: 0.000120 \n",
            "test loss: 0.000902 \n",
            "train loss: 0.000102 \n",
            "test loss: 0.000881 \n",
            "train loss: 0.000101 \n",
            "test loss: 0.000926 \n",
            "train loss: 0.000103 \n",
            "test loss: 0.000893 \n",
            "train loss: 0.000107 \n",
            "test loss: 0.000881 \n",
            "train loss: 0.000121 \n",
            "test loss: 0.000881 \n",
            "train loss: 0.000090 \n",
            "test loss: 0.000946 \n",
            "train loss: 0.000097 \n",
            "test loss: 0.000786 \n",
            "train loss: 0.000112 \n",
            "test loss: 0.000849 \n",
            "train loss: 0.000136 \n",
            "test loss: 0.000839 \n",
            "train loss: 0.000106 \n",
            "test loss: 0.000862 \n",
            "train loss: 0.000154 \n",
            "test loss: 0.000764 \n",
            "train loss: 0.000093 \n",
            "test loss: 0.000811 \n",
            "train loss: 0.000093 \n",
            "test loss: 0.000832 \n",
            "train loss: 0.000090 \n",
            "test loss: 0.000920 \n",
            "train loss: 0.000114 \n",
            "test loss: 0.000744 \n",
            "train loss: 0.000093 \n",
            "test loss: 0.000779 \n",
            "train loss: 0.000091 \n",
            "test loss: 0.000747 \n",
            "train loss: 0.000084 \n",
            "test loss: 0.000799 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000832 \n",
            "train loss: 0.000085 \n",
            "test loss: 0.000768 \n",
            "train loss: 0.000093 \n",
            "test loss: 0.000774 \n",
            "train loss: 0.000110 \n",
            "test loss: 0.000859 \n",
            "train loss: 0.000096 \n",
            "test loss: 0.000775 \n",
            "train loss: 0.000073 \n",
            "test loss: 0.000754 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.000815 \n",
            "train loss: 0.000121 \n",
            "test loss: 0.000866 \n",
            "train loss: 0.000131 \n",
            "test loss: 0.000832 \n",
            "train loss: 0.000139 \n",
            "test loss: 0.000837 \n",
            "train loss: 0.000205 \n",
            "test loss: 0.000652 \n",
            "train loss: 0.000096 \n",
            "test loss: 0.000858 \n",
            "train loss: 0.000097 \n",
            "test loss: 0.000721 \n",
            "train loss: 0.000086 \n",
            "test loss: 0.000772 \n",
            "train loss: 0.000095 \n",
            "test loss: 0.000772 \n",
            "train loss: 0.000097 \n",
            "test loss: 0.000759 \n",
            "train loss: 0.000093 \n",
            "test loss: 0.000794 \n",
            "train loss: 0.000095 \n",
            "test loss: 0.000788 \n",
            "train loss: 0.000090 \n",
            "test loss: 0.000819 \n",
            "train loss: 0.000092 \n",
            "test loss: 0.000822 \n",
            "train loss: 0.000087 \n",
            "test loss: 0.000840 \n",
            "train loss: 0.000087 \n",
            "test loss: 0.000848 \n",
            "train loss: 0.000085 \n",
            "test loss: 0.000859 \n",
            "train loss: 0.000084 \n",
            "test loss: 0.000865 \n",
            "train loss: 0.000082 \n",
            "test loss: 0.000872 \n",
            "train loss: 0.000082 \n",
            "test loss: 0.000876 \n",
            "train loss: 0.000082 \n",
            "test loss: 0.000883 \n",
            "train loss: 0.000082 \n",
            "test loss: 0.000880 \n",
            "train loss: 0.000083 \n",
            "test loss: 0.000896 \n",
            "train loss: 0.000083 \n",
            "test loss: 0.000879 \n",
            "train loss: 0.000082 \n",
            "test loss: 0.000907 \n",
            "train loss: 0.000086 \n",
            "test loss: 0.000889 \n",
            "train loss: 0.000080 \n",
            "test loss: 0.000897 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.000939 \n",
            "train loss: 0.000099 \n",
            "test loss: 0.000885 \n",
            "train loss: 0.000082 \n",
            "test loss: 0.000963 \n",
            "train loss: 0.000145 \n",
            "test loss: 0.000933 \n",
            "train loss: 0.000080 \n",
            "test loss: 0.000925 \n",
            "train loss: 0.000085 \n",
            "test loss: 0.000947 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000908 \n",
            "train loss: 0.000072 \n",
            "test loss: 0.000844 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.000962 \n",
            "train loss: 0.000097 \n",
            "test loss: 0.000868 \n",
            "train loss: 0.000104 \n",
            "test loss: 0.000997 \n",
            "train loss: 0.000111 \n",
            "test loss: 0.000967 \n",
            "train loss: 0.000126 \n",
            "test loss: 0.000987 \n",
            "train loss: 0.000130 \n",
            "test loss: 0.000943 \n",
            "train loss: 0.000143 \n",
            "test loss: 0.000866 \n",
            "train loss: 0.000107 \n",
            "test loss: 0.000842 \n",
            "train loss: 0.000130 \n",
            "test loss: 0.000859 \n",
            "train loss: 0.000158 \n",
            "test loss: 0.000847 \n",
            "train loss: 0.000249 \n",
            "test loss: 0.000878 \n",
            "train loss: 0.000084 \n",
            "test loss: 0.001009 \n",
            "train loss: 0.000076 \n",
            "test loss: 0.001119 \n",
            "train loss: 0.000114 \n",
            "test loss: 0.001140 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.001181 \n",
            "train loss: 0.000133 \n",
            "test loss: 0.001102 \n",
            "train loss: 0.000107 \n",
            "test loss: 0.001230 \n",
            "train loss: 0.000154 \n",
            "test loss: 0.001011 \n",
            "train loss: 0.000125 \n",
            "test loss: 0.000919 \n",
            "train loss: 0.000134 \n",
            "test loss: 0.000858 \n",
            "train loss: 0.000134 \n",
            "test loss: 0.000822 \n",
            "train loss: 0.000162 \n",
            "test loss: 0.000793 \n",
            "train loss: 0.000165 \n",
            "test loss: 0.000835 \n",
            "train loss: 0.000240 \n",
            "test loss: 0.000926 \n",
            "train loss: 0.000158 \n",
            "test loss: 0.000984 \n",
            "train loss: 0.000076 \n",
            "test loss: 0.001196 \n",
            "train loss: 0.000109 \n",
            "test loss: 0.001069 \n",
            "train loss: 0.000084 \n",
            "test loss: 0.001100 \n",
            "train loss: 0.000094 \n",
            "test loss: 0.001187 \n",
            "train loss: 0.000090 \n",
            "test loss: 0.001055 \n",
            "train loss: 0.000123 \n",
            "test loss: 0.000995 \n",
            "train loss: 0.000080 \n",
            "test loss: 0.000974 \n",
            "train loss: 0.000112 \n",
            "test loss: 0.001092 \n",
            "train loss: 0.000078 \n",
            "test loss: 0.000992 \n",
            "train loss: 0.000131 \n",
            "test loss: 0.001046 \n",
            "train loss: 0.000080 \n",
            "test loss: 0.001037 \n",
            "train loss: 0.000101 \n",
            "test loss: 0.001060 \n",
            "train loss: 0.000084 \n",
            "test loss: 0.001021 \n",
            "train loss: 0.000112 \n",
            "test loss: 0.000993 \n",
            "train loss: 0.000098 \n",
            "test loss: 0.001067 \n",
            "train loss: 0.000070 \n",
            "test loss: 0.001107 \n",
            "train loss: 0.000163 \n",
            "test loss: 0.001047 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.001104 \n",
            "train loss: 0.000089 \n",
            "test loss: 0.001119 \n",
            "train loss: 0.000118 \n",
            "test loss: 0.001122 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.001114 \n",
            "train loss: 0.000118 \n",
            "test loss: 0.001178 \n",
            "train loss: 0.000074 \n",
            "test loss: 0.001285 \n",
            "train loss: 0.000102 \n",
            "test loss: 0.001034 \n",
            "train loss: 0.000104 \n",
            "test loss: 0.001084 \n",
            "train loss: 0.000092 \n",
            "test loss: 0.000964 \n",
            "train loss: 0.000113 \n",
            "test loss: 0.001080 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.001140 \n",
            "train loss: 0.000115 \n",
            "test loss: 0.001268 \n",
            "train loss: 0.000079 \n",
            "test loss: 0.001252 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.001268 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.001283 \n",
            "train loss: 0.000054 \n",
            "test loss: 0.001253 \n",
            "train loss: 0.000083 \n",
            "test loss: 0.001218 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.001306 \n",
            "train loss: 0.000069 \n",
            "test loss: 0.001214 \n",
            "train loss: 0.000063 \n",
            "test loss: 0.001300 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.001278 \n",
            "train loss: 0.000074 \n",
            "test loss: 0.001236 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.001347 \n",
            "train loss: 0.000128 \n",
            "test loss: 0.001199 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.001328 \n",
            "train loss: 0.000139 \n",
            "test loss: 0.001312 \n",
            "train loss: 0.000075 \n",
            "test loss: 0.001051 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.001227 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.001041 \n",
            "train loss: 0.000091 \n",
            "test loss: 0.001145 \n",
            "train loss: 0.000152 \n",
            "test loss: 0.001030 \n",
            "train loss: 0.000079 \n",
            "test loss: 0.001214 \n",
            "train loss: 0.000085 \n",
            "test loss: 0.001146 \n",
            "train loss: 0.000081 \n",
            "test loss: 0.001269 \n",
            "train loss: 0.000074 \n",
            "test loss: 0.001164 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.001157 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.001143 \n",
            "train loss: 0.000095 \n",
            "test loss: 0.001260 \n",
            "train loss: 0.000097 \n",
            "test loss: 0.001083 \n",
            "train loss: 0.000069 \n",
            "test loss: 0.001081 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.001106 \n",
            "train loss: 0.000087 \n",
            "test loss: 0.001188 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.001208 \n",
            "train loss: 0.000073 \n",
            "test loss: 0.000994 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000937 \n",
            "train loss: 0.000073 \n",
            "test loss: 0.000829 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000874 \n",
            "train loss: 0.000070 \n",
            "test loss: 0.000875 \n",
            "train loss: 0.000048 \n",
            "test loss: 0.000974 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.001010 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000913 \n",
            "train loss: 0.000082 \n",
            "test loss: 0.000956 \n",
            "train loss: 0.000075 \n",
            "test loss: 0.000954 \n",
            "train loss: 0.000063 \n",
            "test loss: 0.000857 \n",
            "train loss: 0.000078 \n",
            "test loss: 0.000828 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.000853 \n",
            "train loss: 0.000044 \n",
            "test loss: 0.000844 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000839 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.000890 \n",
            "train loss: 0.000102 \n",
            "test loss: 0.000839 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.000731 \n",
            "train loss: 0.000076 \n",
            "test loss: 0.000736 \n",
            "train loss: 0.000052 \n",
            "test loss: 0.000700 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.000709 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000632 \n",
            "train loss: 0.000104 \n",
            "test loss: 0.000697 \n",
            "train loss: 0.000087 \n",
            "test loss: 0.000665 \n",
            "train loss: 0.000081 \n",
            "test loss: 0.000595 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000586 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.000550 \n",
            "train loss: 0.000076 \n",
            "test loss: 0.000571 \n",
            "train loss: 0.000085 \n",
            "test loss: 0.000636 \n",
            "train loss: 0.000099 \n",
            "test loss: 0.000574 \n",
            "train loss: 0.000070 \n",
            "test loss: 0.000568 \n",
            "train loss: 0.000047 \n",
            "test loss: 0.000599 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.000629 \n",
            "train loss: 0.000093 \n",
            "test loss: 0.000698 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000645 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000679 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.000619 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000672 \n",
            "train loss: 0.000054 \n",
            "test loss: 0.000618 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.000654 \n",
            "train loss: 0.000046 \n",
            "test loss: 0.000729 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000671 \n",
            "train loss: 0.000069 \n",
            "test loss: 0.000725 \n",
            "train loss: 0.000081 \n",
            "test loss: 0.000716 \n",
            "train loss: 0.000085 \n",
            "test loss: 0.000691 \n",
            "train loss: 0.000102 \n",
            "test loss: 0.000626 \n",
            "train loss: 0.000120 \n",
            "test loss: 0.000671 \n",
            "train loss: 0.000130 \n",
            "test loss: 0.000800 \n",
            "train loss: 0.000110 \n",
            "test loss: 0.000638 \n",
            "train loss: 0.000195 \n",
            "test loss: 0.000703 \n",
            "train loss: 0.000158 \n",
            "test loss: 0.000948 \n",
            "train loss: 0.000227 \n",
            "test loss: 0.000721 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.000850 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.000763 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000808 \n",
            "train loss: 0.000069 \n",
            "test loss: 0.000795 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000809 \n",
            "train loss: 0.000070 \n",
            "test loss: 0.000799 \n",
            "train loss: 0.000069 \n",
            "test loss: 0.000808 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000801 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000804 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000802 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000804 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000805 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000807 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000809 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000812 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000814 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000817 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000820 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000823 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000826 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000828 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000831 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000833 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.000838 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.000838 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.000846 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000845 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000852 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000851 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.000856 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.000858 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.000855 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000871 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.000848 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000884 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.000858 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000868 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000890 \n",
            "train loss: 0.000058 \n",
            "test loss: 0.000845 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.000898 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.000801 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.000966 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.000813 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000937 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.000768 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000877 \n",
            "train loss: 0.000058 \n",
            "test loss: 0.000805 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000959 \n",
            "train loss: 0.000052 \n",
            "test loss: 0.000951 \n",
            "train loss: 0.000052 \n",
            "test loss: 0.001052 \n",
            "train loss: 0.000054 \n",
            "test loss: 0.000939 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000836 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000855 \n",
            "train loss: 0.000054 \n",
            "test loss: 0.000820 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000838 \n",
            "train loss: 0.000052 \n",
            "test loss: 0.000871 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.000885 \n",
            "train loss: 0.000045 \n",
            "test loss: 0.000842 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000920 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.000995 \n",
            "train loss: 0.000064 \n",
            "test loss: 0.001017 \n",
            "train loss: 0.000048 \n",
            "test loss: 0.001062 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000881 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000812 \n",
            "train loss: 0.000084 \n",
            "test loss: 0.000944 \n",
            "train loss: 0.000108 \n",
            "test loss: 0.000840 \n",
            "train loss: 0.000058 \n",
            "test loss: 0.000864 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000882 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000857 \n",
            "train loss: 0.000046 \n",
            "test loss: 0.000914 \n",
            "train loss: 0.000048 \n",
            "test loss: 0.000787 \n",
            "train loss: 0.000052 \n",
            "test loss: 0.000927 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.000790 \n",
            "train loss: 0.000049 \n",
            "test loss: 0.000938 \n",
            "train loss: 0.000046 \n",
            "test loss: 0.000812 \n",
            "train loss: 0.000051 \n",
            "test loss: 0.000934 \n",
            "train loss: 0.000045 \n",
            "test loss: 0.000793 \n",
            "train loss: 0.000047 \n",
            "test loss: 0.000913 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000793 \n",
            "train loss: 0.000044 \n",
            "test loss: 0.000809 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.000821 \n",
            "train loss: 0.000047 \n",
            "test loss: 0.000915 \n",
            "train loss: 0.000039 \n",
            "test loss: 0.000828 \n",
            "train loss: 0.000047 \n",
            "test loss: 0.000911 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.000782 \n",
            "train loss: 0.000045 \n",
            "test loss: 0.000831 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.000838 \n",
            "train loss: 0.000042 \n",
            "test loss: 0.000876 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000823 \n",
            "train loss: 0.000042 \n",
            "test loss: 0.000863 \n",
            "train loss: 0.000053 \n",
            "test loss: 0.000970 \n",
            "train loss: 0.000080 \n",
            "test loss: 0.000964 \n",
            "train loss: 0.000074 \n",
            "test loss: 0.000829 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000856 \n",
            "train loss: 0.000135 \n",
            "test loss: 0.000749 \n",
            "train loss: 0.000086 \n",
            "test loss: 0.000845 \n",
            "train loss: 0.000063 \n",
            "test loss: 0.000690 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.000862 \n",
            "train loss: 0.000052 \n",
            "test loss: 0.001035 \n",
            "train loss: 0.000044 \n",
            "test loss: 0.000838 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.001004 \n",
            "train loss: 0.000045 \n",
            "test loss: 0.000777 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.000793 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.000737 \n",
            "train loss: 0.000033 \n",
            "test loss: 0.000742 \n",
            "train loss: 0.000029 \n",
            "test loss: 0.000790 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000863 \n",
            "train loss: 0.000038 \n",
            "test loss: 0.000857 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.000884 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.000871 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.000834 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000877 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.000749 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.000738 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.000920 \n",
            "train loss: 0.000054 \n",
            "test loss: 0.000813 \n",
            "train loss: 0.000106 \n",
            "test loss: 0.000881 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.000847 \n",
            "train loss: 0.000054 \n",
            "test loss: 0.000823 \n",
            "train loss: 0.000086 \n",
            "test loss: 0.000799 \n",
            "train loss: 0.000048 \n",
            "test loss: 0.000885 \n",
            "train loss: 0.000038 \n",
            "test loss: 0.000900 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.000839 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000840 \n",
            "train loss: 0.000069 \n",
            "test loss: 0.000878 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.000843 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.000903 \n",
            "train loss: 0.000033 \n",
            "test loss: 0.000900 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.000931 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.000950 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.000866 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.000957 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000950 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000857 \n",
            "train loss: 0.000052 \n",
            "test loss: 0.000923 \n",
            "train loss: 0.000039 \n",
            "test loss: 0.000878 \n",
            "train loss: 0.000029 \n",
            "test loss: 0.000870 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.000878 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.000871 \n",
            "train loss: 0.000099 \n",
            "test loss: 0.000892 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.000869 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.000975 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.000989 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.000988 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.001056 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.001108 \n",
            "train loss: 0.000086 \n",
            "test loss: 0.001019 \n",
            "train loss: 0.000096 \n",
            "test loss: 0.000999 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.000807 \n",
            "train loss: 0.000039 \n",
            "test loss: 0.000786 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000701 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.000812 \n",
            "train loss: 0.000076 \n",
            "test loss: 0.000940 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.001007 \n",
            "train loss: 0.000089 \n",
            "test loss: 0.000954 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.000949 \n",
            "train loss: 0.000066 \n",
            "test loss: 0.000956 \n",
            "train loss: 0.000029 \n",
            "test loss: 0.000901 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000968 \n",
            "train loss: 0.000034 \n",
            "test loss: 0.000916 \n",
            "train loss: 0.000069 \n",
            "test loss: 0.001003 \n",
            "train loss: 0.000033 \n",
            "test loss: 0.000944 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.001028 \n",
            "train loss: 0.000032 \n",
            "test loss: 0.000960 \n",
            "train loss: 0.000032 \n",
            "test loss: 0.001037 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.001044 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.001149 \n",
            "train loss: 0.000027 \n",
            "test loss: 0.001092 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.001147 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.001096 \n",
            "train loss: 0.000051 \n",
            "test loss: 0.001174 \n",
            "train loss: 0.000061 \n",
            "test loss: 0.001127 \n",
            "train loss: 0.000057 \n",
            "test loss: 0.001112 \n",
            "train loss: 0.000051 \n",
            "test loss: 0.001230 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.001246 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.001358 \n",
            "train loss: 0.000058 \n",
            "test loss: 0.001272 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.001242 \n",
            "train loss: 0.000045 \n",
            "test loss: 0.001289 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.001384 \n",
            "train loss: 0.000056 \n",
            "test loss: 0.001359 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.001292 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.001311 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.001176 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.001174 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.001316 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.001369 \n",
            "train loss: 0.000168 \n",
            "test loss: 0.001546 \n",
            "train loss: 0.000047 \n",
            "test loss: 0.001363 \n",
            "train loss: 0.000039 \n",
            "test loss: 0.000877 \n",
            "train loss: 0.000111 \n",
            "test loss: 0.001133 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000881 \n",
            "train loss: 0.000103 \n",
            "test loss: 0.001048 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000879 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.000916 \n",
            "train loss: 0.000042 \n",
            "test loss: 0.000857 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.000913 \n",
            "train loss: 0.000051 \n",
            "test loss: 0.000814 \n",
            "train loss: 0.000034 \n",
            "test loss: 0.000885 \n",
            "train loss: 0.000060 \n",
            "test loss: 0.000841 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.000861 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000873 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.000861 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000884 \n",
            "train loss: 0.000032 \n",
            "test loss: 0.000886 \n",
            "train loss: 0.000038 \n",
            "test loss: 0.000902 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.000906 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.000928 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.000922 \n",
            "train loss: 0.000027 \n",
            "test loss: 0.000958 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.000941 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.000991 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.000940 \n",
            "train loss: 0.000024 \n",
            "test loss: 0.001002 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.000933 \n",
            "train loss: 0.000023 \n",
            "test loss: 0.001012 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.000935 \n",
            "train loss: 0.000025 \n",
            "test loss: 0.001014 \n",
            "train loss: 0.000032 \n",
            "test loss: 0.000932 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.001007 \n",
            "train loss: 0.000029 \n",
            "test loss: 0.000918 \n",
            "train loss: 0.000032 \n",
            "test loss: 0.000985 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.000907 \n",
            "train loss: 0.000025 \n",
            "test loss: 0.000936 \n",
            "train loss: 0.000039 \n",
            "test loss: 0.000953 \n",
            "train loss: 0.000025 \n",
            "test loss: 0.000934 \n",
            "train loss: 0.000027 \n",
            "test loss: 0.000984 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.000955 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.001023 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.000857 \n",
            "train loss: 0.000034 \n",
            "test loss: 0.000955 \n",
            "train loss: 0.000058 \n",
            "test loss: 0.000667 \n",
            "train loss: 0.000073 \n",
            "test loss: 0.000860 \n",
            "train loss: 0.000132 \n",
            "test loss: 0.000724 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.000882 \n",
            "train loss: 0.000093 \n",
            "test loss: 0.000836 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000976 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.001018 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.000931 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.000870 \n",
            "train loss: 0.000024 \n",
            "test loss: 0.000824 \n",
            "train loss: 0.000023 \n",
            "test loss: 0.000870 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.000818 \n",
            "train loss: 0.000032 \n",
            "test loss: 0.000853 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.000900 \n",
            "train loss: 0.000033 \n",
            "test loss: 0.000915 \n",
            "train loss: 0.000033 \n",
            "test loss: 0.000795 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000905 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.000676 \n",
            "train loss: 0.000023 \n",
            "test loss: 0.000789 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.000771 \n",
            "train loss: 0.000045 \n",
            "test loss: 0.000821 \n",
            "train loss: 0.000029 \n",
            "test loss: 0.001018 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.000949 \n",
            "train loss: 0.000070 \n",
            "test loss: 0.000890 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.000805 \n",
            "train loss: 0.000034 \n",
            "test loss: 0.000700 \n",
            "train loss: 0.000046 \n",
            "test loss: 0.000911 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.001045 \n",
            "train loss: 0.000025 \n",
            "test loss: 0.000945 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.000924 \n",
            "train loss: 0.000034 \n",
            "test loss: 0.000887 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.000778 \n",
            "train loss: 0.000021 \n",
            "test loss: 0.000811 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.000848 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.000781 \n",
            "train loss: 0.000025 \n",
            "test loss: 0.000919 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.000900 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.000813 \n",
            "train loss: 0.000033 \n",
            "test loss: 0.000806 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.000773 \n",
            "train loss: 0.000032 \n",
            "test loss: 0.000918 \n",
            "train loss: 0.000032 \n",
            "test loss: 0.000815 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.000926 \n",
            "train loss: 0.000051 \n",
            "test loss: 0.000979 \n",
            "train loss: 0.000024 \n",
            "test loss: 0.000985 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000939 \n",
            "train loss: 0.000029 \n",
            "test loss: 0.000905 \n",
            "train loss: 0.000027 \n",
            "test loss: 0.000990 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.000972 \n",
            "train loss: 0.000039 \n",
            "test loss: 0.000920 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.001011 \n",
            "train loss: 0.000052 \n",
            "test loss: 0.000855 \n",
            "train loss: 0.000034 \n",
            "test loss: 0.000823 \n",
            "train loss: 0.000039 \n",
            "test loss: 0.000700 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.000902 \n",
            "train loss: 0.000082 \n",
            "test loss: 0.000978 \n",
            "train loss: 0.000027 \n",
            "test loss: 0.000846 \n",
            "train loss: 0.000029 \n",
            "test loss: 0.000921 \n",
            "train loss: 0.000052 \n",
            "test loss: 0.000980 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.001049 \n",
            "train loss: 0.000024 \n",
            "test loss: 0.001044 \n",
            "train loss: 0.000044 \n",
            "test loss: 0.001064 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.000947 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.000999 \n",
            "train loss: 0.000033 \n",
            "test loss: 0.000876 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000997 \n",
            "train loss: 0.000027 \n",
            "test loss: 0.000932 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000999 \n",
            "train loss: 0.000029 \n",
            "test loss: 0.001033 \n",
            "train loss: 0.000032 \n",
            "test loss: 0.001093 \n",
            "train loss: 0.000053 \n",
            "test loss: 0.001111 \n",
            "train loss: 0.000034 \n",
            "test loss: 0.001143 \n",
            "train loss: 0.000047 \n",
            "test loss: 0.001115 \n",
            "train loss: 0.000027 \n",
            "test loss: 0.001059 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.000949 \n",
            "train loss: 0.000038 \n",
            "test loss: 0.000953 \n",
            "train loss: 0.000038 \n",
            "test loss: 0.000991 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.001096 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.001126 \n",
            "train loss: 0.000039 \n",
            "test loss: 0.001177 \n",
            "train loss: 0.000033 \n",
            "test loss: 0.001073 \n",
            "train loss: 0.000039 \n",
            "test loss: 0.001184 \n",
            "train loss: 0.000050 \n",
            "test loss: 0.001125 \n",
            "train loss: 0.000042 \n",
            "test loss: 0.001142 \n",
            "train loss: 0.000049 \n",
            "test loss: 0.001089 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.001241 \n",
            "train loss: 0.000027 \n",
            "test loss: 0.001101 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.001087 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.001141 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.001155 \n",
            "train loss: 0.000033 \n",
            "test loss: 0.001182 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.001120 \n",
            "train loss: 0.000025 \n",
            "test loss: 0.001218 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.001188 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.001274 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.001252 \n",
            "train loss: 0.000023 \n",
            "test loss: 0.001364 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.001363 \n",
            "train loss: 0.000020 \n",
            "test loss: 0.001284 \n",
            "train loss: 0.000020 \n",
            "test loss: 0.001281 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.001179 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.001195 \n",
            "train loss: 0.000029 \n",
            "test loss: 0.001251 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.001239 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.001248 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.001170 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.001436 \n",
            "train loss: 0.000069 \n",
            "test loss: 0.001116 \n",
            "train loss: 0.000071 \n",
            "test loss: 0.001198 \n",
            "train loss: 0.000053 \n",
            "test loss: 0.001166 \n",
            "train loss: 0.000045 \n",
            "test loss: 0.001253 \n",
            "train loss: 0.000077 \n",
            "test loss: 0.001185 \n",
            "train loss: 0.000034 \n",
            "test loss: 0.001132 \n",
            "train loss: 0.000046 \n",
            "test loss: 0.001150 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.001001 \n",
            "train loss: 0.000038 \n",
            "test loss: 0.001225 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000930 \n",
            "train loss: 0.000021 \n",
            "test loss: 0.001028 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.000846 \n",
            "train loss: 0.000024 \n",
            "test loss: 0.000928 \n",
            "train loss: 0.000023 \n",
            "test loss: 0.000839 \n",
            "train loss: 0.000023 \n",
            "test loss: 0.000898 \n",
            "train loss: 0.000032 \n",
            "test loss: 0.000771 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.000880 \n",
            "train loss: 0.000049 \n",
            "test loss: 0.000806 \n",
            "train loss: 0.000023 \n",
            "test loss: 0.000871 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.000890 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.001054 \n",
            "train loss: 0.000053 \n",
            "test loss: 0.001035 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.001050 \n",
            "train loss: 0.000079 \n",
            "test loss: 0.000886 \n",
            "train loss: 0.000047 \n",
            "test loss: 0.000884 \n",
            "train loss: 0.000068 \n",
            "test loss: 0.000849 \n",
            "train loss: 0.000065 \n",
            "test loss: 0.000927 \n",
            "train loss: 0.000042 \n",
            "test loss: 0.000982 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.001131 \n",
            "train loss: 0.000034 \n",
            "test loss: 0.001162 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.001169 \n",
            "train loss: 0.000024 \n",
            "test loss: 0.001112 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.001146 \n",
            "train loss: 0.000025 \n",
            "test loss: 0.001157 \n",
            "train loss: 0.000038 \n",
            "test loss: 0.001120 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.001136 \n",
            "train loss: 0.000063 \n",
            "test loss: 0.001151 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.001047 \n",
            "train loss: 0.000059 \n",
            "test loss: 0.001100 \n",
            "train loss: 0.000042 \n",
            "test loss: 0.000962 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.001105 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.000952 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.001090 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.000908 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.001082 \n",
            "train loss: 0.000025 \n",
            "test loss: 0.000863 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.001036 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.000785 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.000970 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.000771 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.000947 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.000788 \n",
            "train loss: 0.000020 \n",
            "test loss: 0.000993 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.000883 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.001047 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.000945 \n",
            "train loss: 0.000020 \n",
            "test loss: 0.001089 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.000981 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.001093 \n",
            "train loss: 0.000020 \n",
            "test loss: 0.000982 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.001074 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.000968 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.001055 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.000994 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.001068 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.000959 \n",
            "train loss: 0.000021 \n",
            "test loss: 0.001091 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.000922 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.001068 \n",
            "train loss: 0.000021 \n",
            "test loss: 0.000946 \n",
            "train loss: 0.000024 \n",
            "test loss: 0.001078 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.000938 \n",
            "train loss: 0.000024 \n",
            "test loss: 0.001015 \n",
            "train loss: 0.000035 \n",
            "test loss: 0.000956 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000986 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.001037 \n",
            "train loss: 0.000034 \n",
            "test loss: 0.001171 \n",
            "train loss: 0.000042 \n",
            "test loss: 0.001116 \n",
            "train loss: 0.000037 \n",
            "test loss: 0.001152 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.001250 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.001084 \n",
            "train loss: 0.000031 \n",
            "test loss: 0.001038 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.000995 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.001062 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000839 \n",
            "train loss: 0.000024 \n",
            "test loss: 0.000965 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.000910 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.000884 \n",
            "train loss: 0.000021 \n",
            "test loss: 0.000942 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.000967 \n",
            "train loss: 0.000021 \n",
            "test loss: 0.001026 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.001085 \n",
            "train loss: 0.000038 \n",
            "test loss: 0.000941 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.001026 \n",
            "train loss: 0.000016 \n",
            "test loss: 0.001014 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.001056 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.000897 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.001070 \n",
            "train loss: 0.000015 \n",
            "test loss: 0.000930 \n",
            "train loss: 0.000014 \n",
            "test loss: 0.000942 \n",
            "train loss: 0.000014 \n",
            "test loss: 0.000826 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.000890 \n",
            "train loss: 0.000016 \n",
            "test loss: 0.000869 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.000939 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.000842 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.000855 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.000857 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.000896 \n",
            "train loss: 0.000014 \n",
            "test loss: 0.000898 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.000941 \n",
            "train loss: 0.000021 \n",
            "test loss: 0.000998 \n",
            "train loss: 0.000020 \n",
            "test loss: 0.001013 \n",
            "train loss: 0.000029 \n",
            "test loss: 0.001004 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.001101 \n",
            "train loss: 0.000032 \n",
            "test loss: 0.000969 \n",
            "train loss: 0.000043 \n",
            "test loss: 0.000880 \n",
            "train loss: 0.000055 \n",
            "test loss: 0.000771 \n",
            "train loss: 0.000277 \n",
            "test loss: 0.000986 \n",
            "train loss: 0.000054 \n",
            "test loss: 0.000831 \n",
            "train loss: 0.000044 \n",
            "test loss: 0.000843 \n",
            "train loss: 0.000033 \n",
            "test loss: 0.000862 \n",
            "train loss: 0.000025 \n",
            "test loss: 0.000906 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.000930 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.000956 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.000968 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.000982 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.000993 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.001004 \n",
            "train loss: 0.000016 \n",
            "test loss: 0.001012 \n",
            "train loss: 0.000016 \n",
            "test loss: 0.001022 \n",
            "train loss: 0.000015 \n",
            "test loss: 0.001030 \n",
            "train loss: 0.000015 \n",
            "test loss: 0.001039 \n",
            "train loss: 0.000014 \n",
            "test loss: 0.001046 \n",
            "train loss: 0.000014 \n",
            "test loss: 0.001056 \n",
            "train loss: 0.000014 \n",
            "test loss: 0.001061 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.001070 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.001074 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.001082 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001084 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001092 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001091 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001100 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001094 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001106 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001093 \n",
            "train loss: 0.000011 \n",
            "test loss: 0.001110 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001084 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.001109 \n",
            "train loss: 0.000016 \n",
            "test loss: 0.001066 \n",
            "train loss: 0.000016 \n",
            "test loss: 0.001090 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.001070 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.001084 \n",
            "train loss: 0.000016 \n",
            "test loss: 0.001044 \n",
            "train loss: 0.000023 \n",
            "test loss: 0.001122 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.001000 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.001153 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001078 \n",
            "train loss: 0.000016 \n",
            "test loss: 0.001180 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.001127 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001189 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001157 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001170 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.001155 \n",
            "train loss: 0.000014 \n",
            "test loss: 0.001160 \n",
            "train loss: 0.000014 \n",
            "test loss: 0.001127 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.001138 \n",
            "train loss: 0.000014 \n",
            "test loss: 0.001089 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.001103 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.001063 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.001100 \n",
            "train loss: 0.000010 \n",
            "test loss: 0.001061 \n",
            "train loss: 0.000015 \n",
            "test loss: 0.001108 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.001125 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.001121 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.001137 \n",
            "train loss: 0.000025 \n",
            "test loss: 0.001176 \n",
            "train loss: 0.000044 \n",
            "test loss: 0.001151 \n",
            "train loss: 0.000015 \n",
            "test loss: 0.001264 \n",
            "train loss: 0.000062 \n",
            "test loss: 0.001149 \n",
            "train loss: 0.000023 \n",
            "test loss: 0.001227 \n",
            "train loss: 0.000041 \n",
            "test loss: 0.001173 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.001501 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.001260 \n",
            "train loss: 0.000067 \n",
            "test loss: 0.001255 \n",
            "train loss: 0.000045 \n",
            "test loss: 0.000958 \n",
            "train loss: 0.000053 \n",
            "test loss: 0.000776 \n",
            "train loss: 0.000047 \n",
            "test loss: 0.000891 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.000861 \n",
            "train loss: 0.000030 \n",
            "test loss: 0.000987 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.000964 \n",
            "train loss: 0.000026 \n",
            "test loss: 0.000883 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.000859 \n",
            "train loss: 0.000018 \n",
            "test loss: 0.000891 \n",
            "train loss: 0.000019 \n",
            "test loss: 0.000900 \n",
            "train loss: 0.000015 \n",
            "test loss: 0.000926 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.000879 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.000876 \n",
            "train loss: 0.000014 \n",
            "test loss: 0.000854 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.000868 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.000889 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.000894 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.000929 \n",
            "train loss: 0.000011 \n",
            "test loss: 0.000916 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.000982 \n",
            "train loss: 0.000011 \n",
            "test loss: 0.000954 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001038 \n",
            "train loss: 0.000013 \n",
            "test loss: 0.001025 \n",
            "train loss: 0.000011 \n",
            "test loss: 0.001109 \n",
            "train loss: 0.000014 \n",
            "test loss: 0.001068 \n",
            "train loss: 0.000012 \n",
            "test loss: 0.001122 \n",
            "train loss: 0.000022 \n",
            "test loss: 0.001083 \n",
            "train loss: 0.000023 \n",
            "test loss: 0.001068 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.001073 \n",
            "train loss: 0.000040 \n",
            "test loss: 0.001021 \n",
            "train loss: 0.000027 \n",
            "test loss: 0.001006 \n",
            "train loss: 0.000036 \n",
            "test loss: 0.001107 \n",
            "train loss: 0.000044 \n",
            "test loss: 0.001053 \n",
            "train loss: 0.000023 \n",
            "test loss: 0.001087 \n",
            "train loss: 0.000029 \n",
            "test loss: 0.001020 \n",
            "train loss: 0.000028 \n",
            "test loss: 0.001052 \n",
            "train loss: 0.000017 \n",
            "test loss: 0.001150 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572bd614",
      "metadata": {
        "id": "572bd614"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "def train(dataloader):\n",
        "    hn , cn = model.init()\n",
        "    model.train()\n",
        "    for batch , item in enumerate(dataloader):\n",
        "        x , y = item\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        out , hn , cn = model(x.reshape(100,batch_size,1),hn,cn)\n",
        "        loss = loss_fn(out.reshape(batch_size) , y)\n",
        "        hn = hn.detach()\n",
        "        cn = cn.detach()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch == len(dataloader)-1:\n",
        "            loss = loss.item()\n",
        "            print(f\"train loss: {loss:>7f} \")\n",
        "def test(dataloader):\n",
        "    hn , cn = model.init()\n",
        "    model.eval()\n",
        "    for batch , item in enumerate(dataloader):\n",
        "        x , y = item\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        out , hn , cn = model(x.reshape(100,batch_size,1),hn,cn)\n",
        "        loss = loss_fn(out.reshape(batch_size) , y)\n",
        "       \n",
        "        if batch == len(dataloader)-1:\n",
        "            loss = loss.item()\n",
        "            print(f\"test loss: {loss:>7f} \")\n",
        "epochs = 10000\n",
        "for epoch in range(epochs):\n",
        "  if epoch %500000 == 0:\n",
        "      print(f\"epoch {epoch} \")\n",
        "      train(train_dataloader)\n",
        "      test(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2a4d7136",
      "metadata": {
        "id": "2a4d7136",
        "outputId": "46ee782f-1b55-4852-f904-8543b7609d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "815.152 818.8821\n",
            " r2_score = 0.9850988882235076\n",
            "train mse loss 446.0507783593702\n",
            "2368.7288 1711.8489\n",
            " r2_score = 0.4326493520032164\n",
            "test mse loss 1742.68184130093\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "import numpy as np\n",
        "def calculate_metrics(data_loader):\n",
        "    pred_arr = []\n",
        "    y_arr = []\n",
        "    with torch.no_grad():\n",
        "        hn , cn = model.init()\n",
        "        for batch , item in enumerate(data_loader):\n",
        "            x , y = item\n",
        "            x , y = x.to(device) , y.to(device)\n",
        "            x = x.view(100,64,1)\n",
        "            pred = model(x,hn,cn)[0]\n",
        "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
        "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(1,-1)).reshape(-1)\n",
        "            pred_arr = pred_arr + list(pred)\n",
        "            y_arr = y_arr + list(y)\n",
        "        print(pred_arr[500],y_arr[500])\n",
        "        print(f\" r2_score = {r2_score(y_arr, pred_arr)}\")\n",
        "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
        "    \n",
        "# calculating final loss metrics\n",
        "print(f\"train mse loss {calculate_metrics(train_dataloader)}\")\n",
        "print(f\"test mse loss {calculate_metrics(test_dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4fbz74ar5YWr"
      },
      "id": "4fbz74ar5YWr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}